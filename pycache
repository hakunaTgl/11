     *.pyc
     .models/Mask mask mask   - Create the main script:
     ```python
     from transformers import AutoModelForCausalLM, AutoTokenizer
     import torch
     # Load the model and tokenizer
     model_path = "models/wizard_vicuna_13b_uncensored.gguf"
     tokenizer = AutoTokenizer.from_pretrained(model_path)
     model = AutoModelForCausalLM.from_pretrained(model_path)
     def generate_response(prompt):
         inputs = tokenizer(prompt, return_tensors="pt")
         outputs = model.generate(**inputs, max_length=150)
         response = tokenizer.decode(outputs[0], skip_special_tokens=True)
         return response
     def main():
         print("Welcome to the Uncensored AI Agent!")
         while True:
             user_input = input("You: ")
             if user_input.lower() in ["exit", "quit"]:
                 break
             response = generate_response(user_input)
             print(f"AI Agent: {response}")
     if name == "__main__":
         main()
     ```
7. src/tokenizer.py:
   - Create a script for tokenizer utilities:
     ```python
     from transformers import AutoTokenizer
     def load_tokenizer(model_path):
         return AutoTokenizer.from_pretrained(model_path)
     ```
